## Reproducing Experimental Results

This section describes how to reproduce the experimental results reported in our ICSE’26 paper *“DeFT: Maintaining Determinism and Extracting Module Tests for Autonomous Driving Planning”*.

Our evaluation follows the same workflow described in Section 5 of the paper and consists of generating system-level scenario tests, extracting planning module tests using DeFT, and comparing determinism and efficiency.

---

### Step 0: Generate Initial Scenario Test Suite

We evaluate DeFT using scenarios generated by existing state-of-the-art ADS test generators.

- **Decictor** (ICSE’25)  
  https://github.com/MingfeiCheng/Decictor

- **DoppelTest** (ICSE’23)  
  https://github.com/Software-Aurora-Lab/DoppelTest

- **scenoRITA** (TSE’23)  
  https://github.com/Software-Aurora-Lab/scenoRITA-7.0

Please follow each tool’s official instructions to generate scenario specifications compatible with Apollo 7.0.

> Note: Some original implementations target different ADS versions or simulators. We used implementation provided by Decictor for our evaluation.

---

### Step 1: Execute System-Level Scenario Tests

For each generated scenario `S`, execute it **10 times**:

```bash
# Example pseudocode
for i in {0..9}
do
    run_scenario S
done
```

Denote the executions as:

- `S0` → initial execution
- `S1`–`S9` → reruns

Record:

- Planning module outputs (`/apollo/planning`)
- System-level outcomes (e.g., collision or no collision)

In our paper, we observed that:

- 658 scenarios initially revealed collisions.
- Only 471 failures were reproducible across all reruns (71.58%).

This establishes the baseline non-determinism of system-level scenario testing.

---

### Step 2: Extract Planning Module Tests Using DeFT

From the initial execution `S0`:

1. Record all message bus traffic.
2. Run DeFT to:
   - Filter planning-related messages
   - Reconstruct input frames
   - Validate planning trajectory similarity
   - Produce deterministic module tests

```bash
poetry run python extract_and_execute.py
```

Each system-level scenario produces multiple planning module tests,
each corresponding to one planning execution frame.

---

### Step 3: Evaluate Planning Reproducibility (RQ1)

To reproduce RQ1 results:

1. Compare planning trajectories from:
   - System reruns (`S1`–`S9`)
   - DeFT module test executions

2. Measure trajectory similarity using lock-step Euclidean distance (LSED).

In the paper, we show:

- DeFT reproduces significantly more planning executions than rerunning system tests.

This demonstrates that DeFT generates realistic and precise module tests.

---

### Step 4: Evaluate Failure Reproducibility (RQ2)

For each failing scenario:

1. Rerun:
   - System-level scenario test
   - DeFT-extracted module tests

2. Count how many times failures are reproduced in system-level testing.

3. Count how many times DeFT deterministically reproduced initial planning failure.

In our results:

- System-level reruns reproduced 85.71%–90.88% of failures per run.
- Only 471 failures (71.58%) were deterministic across all reruns.
- DeFT deterministically reproduce planning trajectories from all 658 failures across all reruns.

This confirms that DeFT eliminates flakiness in planning-level testing.

---

### Step 5: Evaluate Efficiency (RQ3)

Measure:

- Extraction time per frame
- Module test execution time
- System-level execution time
- Time-To-Reproduce-Failure (TTRF)

Our evaluation shows:

- DeFT reduces Time-To-Reproduce-Failure by **43.69%–77.64%**
- DeFT avoids runtime overhead by reconstructing inputs offline